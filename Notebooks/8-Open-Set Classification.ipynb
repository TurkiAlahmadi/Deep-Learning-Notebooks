{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IIkBdB1pCG6J"
   },
   "source": [
    "# Open-Set Classification\n",
    "-----------------------\n",
    "\n",
    "We select the MNIST dataset and define several classes to be known, known unknown (used as negative class during training) and unknown unknown (not used for training at all)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qlBzb9Ilap22"
   },
   "source": [
    "## Target Vectors\n",
    "\n",
    "For the training dataset, we want to use four classes of MNIST digits (4,5,8,9) as known classes and four (0,2,3,7) as known unknowns.\n",
    "The remaining two classes shall be ignored during training and validation, and only be used for testing.\n",
    "\n",
    "When we want to train with our adapted softmax function, we need to assign the correct target vectors for the classes.\n",
    "\n",
    "These are $(1,0,0,0)$, $(0,1,0,0)$, $(0,0,1,0)$, and $(0,0,0,1)$ for the known classes, respectively.\n",
    "For known unknown classes, the target vector is $\\left(\\frac14,\\frac14,\\frac14,\\frac14\\right)$, throughout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "pmoczEUUCG6N"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# define the three types of classes\n",
    "known_classes = (4,5,8,9)\n",
    "known_unknown_classes = (0,2,3,7)\n",
    "unknown_classes = (1,6)\n",
    "O = len(known_classes)\n",
    "\n",
    "def target_vector(index):\n",
    "  # select correct one-hot vector for known classes, and the 1/O-vectors for unknown classes\n",
    "  if index in known_classes:\n",
    "    pos = known_classes.index(index)\n",
    "    return torch.tensor([1 if i == pos else 0 for i in range(O)])\n",
    "  else:\n",
    "    return torch.tensor([1 / O for _ in range(O)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_q5h4NpDCG6P"
   },
   "source": [
    "## Check Target Vectors\n",
    "\n",
    "Test that the target vectors are correct for all types of known and unknown samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Md-um-x6CG6P"
   },
   "outputs": [],
   "source": [
    "# check that the target vectors for known classes are correct\n",
    "for index in known_classes:\n",
    "  t = target_vector(index)\n",
    "  assert t.sum() == 1\n",
    "  assert t.argmax() == known_classes.index(index)\n",
    "  assert type(t) == torch.Tensor\n",
    "\n",
    "# check that the target vectors for unknown classes are correct\n",
    "for index in known_unknown_classes + unknown_classes:\n",
    "  t = target_vector(index)\n",
    "  assert t.sum() == 1\n",
    "  assert type(t) == torch.Tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YsLWrd3ICG6Q"
   },
   "source": [
    "## Training Dataset\n",
    "\n",
    "We rely on the MNIST dataset implementation from PyTorch and adapt some parts of it.\n",
    "Mainly, we will let PyTorch load the dataset by calling the base class constructor and modify the `self.data` and `self.targets` ourselves.\n",
    "Additionally, we need to implement the index function to return the data and targets in the desired format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "iP8Sf8JxCG6R"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DataSet(torchvision.datasets.MNIST):\n",
    "  def __init__(self, purpose=\"train\"):\n",
    "    # call base class constructor to handle the data loading\n",
    "    super(DataSet, self).__init__(\n",
    "      \".\", train = True if purpose == \"train\" else False, download=True, transform = torchvision.transforms.ToTensor()\n",
    "    )\n",
    "\n",
    "    # select the valid classes based on the current purpose\n",
    "    # select the valid classes based on the current purpose\n",
    "    if purpose == \"train\":\n",
    "      self.classes = known_classes + known_unknown_classes\n",
    "    elif purpose == \"valid\":\n",
    "      self.classes = known_classes + known_unknown_classes\n",
    "    elif purpose == \"test\":\n",
    "      self.classes = known_classes + unknown_classes\n",
    "\n",
    "    # check if one element of the list is in another list\n",
    "    mask = np.column_stack([self.targets == i for i in self.classes]).any(axis=1)\n",
    "    # sub-select the data of valid classes\n",
    "    self.data = self.data[mask]\n",
    "    # select the targets of valid classes\n",
    "    self.targets = self.targets[mask]\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    # perform appropriate actions on the data and the targets\n",
    "    (i,t) = super().__getitem__(index)\n",
    "    target = target_vector(t)\n",
    "    return i, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4LEzAljkap24",
    "outputId": "3809a139-2e5d-48ee-e29b-2b5173f73eca",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./DataSet/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 411266086.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./DataSet/raw/train-images-idx3-ubyte.gz to ./DataSet/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./DataSet/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 96753749.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./DataSet/raw/train-labels-idx1-ubyte.gz to ./DataSet/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./DataSet/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 251954220.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./DataSet/raw/t10k-images-idx3-ubyte.gz to ./DataSet/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./DataSet/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 11462412.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./DataSet/raw/t10k-labels-idx1-ubyte.gz to ./DataSet/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0706, 0.0706, 0.0706,\n",
       "           0.4941, 0.5333, 0.6863, 0.1020, 0.6510, 1.0000, 0.9686, 0.4980,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.1176, 0.1412, 0.3686, 0.6039, 0.6667, 0.9922, 0.9922, 0.9922,\n",
       "           0.9922, 0.9922, 0.8824, 0.6745, 0.9922, 0.9490, 0.7647, 0.2510,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1922,\n",
       "           0.9333, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "           0.9922, 0.9843, 0.3647, 0.3216, 0.3216, 0.2196, 0.1529, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706,\n",
       "           0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765, 0.7137,\n",
       "           0.9686, 0.9451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.3137, 0.6118, 0.4196, 0.9922, 0.9922, 0.8039, 0.0431, 0.0000,\n",
       "           0.1686, 0.6039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0549, 0.0039, 0.6039, 0.9922, 0.3529, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.5451, 0.9922, 0.7451, 0.0078, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0431, 0.7451, 0.9922, 0.2745, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.1373, 0.9451, 0.8824, 0.6275,\n",
       "           0.4235, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3176, 0.9412, 0.9922,\n",
       "           0.9922, 0.4667, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1765, 0.7294,\n",
       "           0.9922, 0.9922, 0.5882, 0.1059, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0627,\n",
       "           0.3647, 0.9882, 0.9922, 0.7333, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.9765, 0.9922, 0.9765, 0.2510, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1804, 0.5098,\n",
       "           0.7176, 0.9922, 0.9922, 0.8118, 0.0078, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.1529, 0.5804, 0.8980, 0.9922,\n",
       "           0.9922, 0.9922, 0.9804, 0.7137, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0941, 0.4471, 0.8667, 0.9922, 0.9922, 0.9922,\n",
       "           0.9922, 0.7882, 0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0902, 0.2588, 0.8353, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765,\n",
       "           0.3176, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.6706,\n",
       "           0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.7647, 0.3137, 0.0353,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.2157, 0.6745, 0.8863, 0.9922,\n",
       "           0.9922, 0.9922, 0.9922, 0.9569, 0.5216, 0.0431, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.5333, 0.9922, 0.9922, 0.9922,\n",
       "           0.8314, 0.5294, 0.5176, 0.0627, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000]]]),\n",
       " tensor([0, 1, 0, 0]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = DataSet(purpose=\"train\")\n",
    "ds.__getitem__(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vlLi9VFCG6S"
   },
   "source": [
    "## Data Sets\n",
    "\n",
    "Instantiate the training dataset.\n",
    "Implement a data loader for the training dataset with a batch size of 64.\n",
    "Assure that all inputs are of the desired type and shape.\n",
    "Assert that the target values are in the correct format, and the sum of the target values per sample is one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "MO0Y-5wTCG6T"
   },
   "outputs": [],
   "source": [
    "# instantiate the training dataset\n",
    "train_set = DataSet(purpose=\"train\")\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "\n",
    "# assert that we have not filtered out all samples\n",
    "assert len(train_set.data) == len(train_set.targets)\n",
    "assert len(train_set.data) > 0\n",
    "\n",
    "# check the batch and assert valid data and sizes\n",
    "for i, (x,t) in enumerate(train_loader):\n",
    "  if i == len(train_loader) - 1:\n",
    "    continue\n",
    "  assert x.shape == (64, 1, 28, 28)\n",
    "  assert t.shape == (64, 4)\n",
    "  assert torch.all(x <= 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9XqJo_RLap26",
    "outputId": "0d0dd6b8-932b-439f-9e13-819a3619a7f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47340"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dib-Vio8CG6T"
   },
   "source": [
    "## Utility Function\n",
    "\n",
    "Implement a function that splits a batch of samples into known and unknown parts. For the known parts, also provide the target vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Ng82VswACG6U"
   },
   "outputs": [],
   "source": [
    "def split_known_unknown(batch, targets):\n",
    "  # select the indexes at which known and unknown samples exist\n",
    "  known = torch.any(targets == 1, dim=1)\n",
    "  unknown = torch.all(targets == 1/4, dim=1)\n",
    "  # return the known samples, the targets of the known samples, as well as the unknown samples\n",
    "  return batch[known], targets[known], batch[unknown]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2GuNoofDCG6U"
   },
   "source": [
    "## Loss Function Implementation\n",
    "\n",
    "Implement a loss function that implements an autograd function, i.e., we define both the forward and the backward pass for our loss computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1dDOjCBXCG6V"
   },
   "outputs": [],
   "source": [
    "class AdaptedSoftMax(torch.autograd.Function):\n",
    "\n",
    "  # implement the forward propagation\n",
    "  @staticmethod\n",
    "  def forward(ctx, logits, targets):\n",
    "    # compute the log probabilities via log_softmax\n",
    "    log_probs = torch.nn.functional.log_softmax(logits,dim=1)\n",
    "    # save required values for backward pass\n",
    "    ctx.save_for_backward(log_probs, targets)\n",
    "    # compute loss\n",
    "    loss = - targets * log_probs\n",
    "    return torch.sum(loss)\n",
    "\n",
    "  # implement Jacobian\n",
    "  @staticmethod\n",
    "  def backward(ctx, result):\n",
    "    # get results stored from forward pass\n",
    "    log_probs, targets = ctx.saved_tensors\n",
    "    # compute derivative of loss w.r.t. the logits\n",
    "    dJ_dy = torch.exp(log_probs) - targets\n",
    "    # return the derivatives; none for derivative for the targets\n",
    "    return dJ_dy, None\n",
    "\n",
    "# DO NOT REMOVE!\n",
    "# here the adapted softmax function is set to be used later\n",
    "adapted_softmax = AdaptedSoftMax.apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gs39yr2OCG6W"
   },
   "source": [
    "## Confidence Evaluation\n",
    "\n",
    "Implement a function to compute the confidence value for a given batch of samples. Make sure to split the batch between known and unknown samples, and compute the confidence value for both separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "yhBEWy_tCG6W"
   },
   "outputs": [],
   "source": [
    "def confidence(logits, targets):\n",
    "  # comupte softmax confidences\n",
    "  batch = torch.nn.functional.softmax(logits, dim=1)\n",
    "  # split between known and unknown\n",
    "  batch_known, targets_known, batch_unknown = split_known_unknown(batch, targets)\n",
    "  # compute confidence score for known targets\n",
    "  conf_known = torch.sum(batch_known * targets_known)\n",
    "  # compute confidence score for unknown targets\n",
    "  conf_unknown = torch.sum(1 - torch.max(batch_unknown, dim = 1)[0] + 1/4)\n",
    "  return conf_known + conf_unknown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Dkd1hqqCG6X"
   },
   "source": [
    "## Check Confidence Implementation\n",
    "\n",
    "Test that the confidence implementation does what it is supposed to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "L9Vdl215CG6X"
   },
   "outputs": [],
   "source": [
    "# select good logit vectors for known and unknown classes\n",
    "known_logits = torch.diag(torch.ones(4)) * 5 - 4\n",
    "unknown_logits = torch.ones((4, 4))\n",
    "logits = torch.vstack((known_logits, unknown_logits))\n",
    "targets = torch.stack([target_vector(c) for c in known_classes + known_unknown_classes])\n",
    "# select the according target vectors for these classes\n",
    "conf = confidence(logits, targets)\n",
    "\n",
    "# the confidence should be close to 1 for all cases\n",
    "assert 5 - conf < 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Um9cmgSCG6X"
   },
   "source": [
    "## Network Definition\n",
    "\n",
    "We define our own small-scale network to classify known and unknown samples for MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "i7HLLbqqCG6Y"
   },
   "outputs": [],
   "source": [
    "class Network (torch.nn.Module):\n",
    "  def __init__(self, Q1, Q2, K, O):\n",
    "    # call base class constrcutor\n",
    "    super(Network,self).__init__()\n",
    "    # define convolutional layers\n",
    "    self.conv1 = torch.nn.Conv2d(1, Q1, (5,5), stride=1, padding=2)\n",
    "    self.conv2 = torch.nn.Conv2d(Q1, Q2, (5,5), stride=1, padding=2)\n",
    "    # pooling and activation functions will be re-used for the different stages\n",
    "    self.pool = torch.nn.MaxPool2d((2,2), 2)\n",
    "    self.act = torch.nn.ReLU()\n",
    "    # define fully-connected layers\n",
    "    self.flatten = torch.nn.Flatten()\n",
    "    self.fc1 = torch.nn.Linear(7*7*Q2, K)\n",
    "    self.fc2 = torch.nn.Linear(K,O)\n",
    "\n",
    "  def forward(self,x):\n",
    "    # compute first layer of convolution, pooling and activation\n",
    "    a = self.conv1(x)\n",
    "    a = self.pool(a)\n",
    "    a = self.act(a)\n",
    "    # compute second layer of convolution, pooling and activation\n",
    "    a = self.conv2(a)\n",
    "    a = self.pool(a)\n",
    "    a = self.act(a)\n",
    "    a = self.flatten(a)\n",
    "\n",
    "    # get the deep features as the output of the first fully-connected layer\n",
    "    deep_features = self.fc1(a)\n",
    "    # get the logits as the output of the second fully-connected layer\n",
    "    logits = self.fc2(deep_features)\n",
    "    # return both the logits and the deep features\n",
    "    return logits, deep_features\n",
    "\n",
    "# run on cuda device\n",
    "device = torch.device(\"cuda\")\n",
    "# create network with 20 hidden neurons in FC layer\n",
    "network = Network(Q1=32, Q2=32, K=20, O=4).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2wlDarXQCG6Y"
   },
   "source": [
    "## Training Loop\n",
    "\n",
    "Implement the training loop for 100 epochs.\n",
    "Compute the running training confidence and validation confidence and print them at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WEhQoA6vCG6Y",
    "outputId": "95d30b60-3566-48c4-a597-8f3822beb723"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0; train: 0.88853, val: 0.93874\n",
      "Epoch 1; train: 0.94658, val: 0.95810\n",
      "Epoch 2; train: 0.95714, val: 0.95519\n",
      "Epoch 3; train: 0.96348, val: 0.96353\n",
      "Epoch 4; train: 0.96721, val: 0.96853\n",
      "Epoch 5; train: 0.97044, val: 0.96970\n",
      "Epoch 6; train: 0.97412, val: 0.97309\n",
      "Epoch 7; train: 0.97579, val: 0.96861\n",
      "Epoch 8; train: 0.97698, val: 0.97522\n",
      "Epoch 9; train: 0.97882, val: 0.97264\n"
     ]
    }
   ],
   "source": [
    "# SGD optimizer with appropriate learning rate\n",
    "optimizer = torch.optim.SGD(network.parameters(),lr=0.001, momentum=0.9)\n",
    "\n",
    "# validation set and data loader\n",
    "validation_set = DataSet(\"valid\")\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=100)\n",
    "\n",
    "for epoch in range(10): # or 100\n",
    "  # evaluate average confidence for training and validation set\n",
    "  train_conf = validation_conf = 0.\n",
    "\n",
    "  for x,t in train_loader:\n",
    "    # extract logits (and deep features) from network\n",
    "    optimizer.zero_grad()\n",
    "    # extract logits (and deep features) from network\n",
    "    logits, deep_feature = network(x.to(device))\n",
    "    # compute our loss\n",
    "    J = AdaptedSoftMax.apply(logits, t.to(device))\n",
    "    #J = adapted_softmax_alt(logits, t.to(device))\n",
    "    J.backward()\n",
    "    # perform weight update\n",
    "    optimizer.step()\n",
    "\n",
    "    # compute training confidence\n",
    "    with torch.no_grad():\n",
    "      train_conf += confidence(logits, t.to(device))\n",
    "\n",
    "  # compute validation comfidence\n",
    "  with torch.no_grad():\n",
    "    for x,t in validation_loader:\n",
    "      # extract logits (and deep features)\n",
    "      logits, deep_feature = network(x.to(device))\n",
    "      # compute validation confidence\n",
    "      validation_conf += confidence(logits, t.to(device))\n",
    "\n",
    "  # print average confidence for training and validation\n",
    "  print(f\"\\rEpoch {epoch}; train: {train_conf/len(train_set):1.5f}, val: {validation_conf/len(validation_set):1.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s2UBmSKUCG6Z"
   },
   "source": [
    "## Feature Magnitude Plot\n",
    "\n",
    "Take the validation and test sets and plot their feature magnitude as histogram, based on the pre-trained network and split between known, known unknown (validation set) and unknown unknown (test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 234
    },
    "id": "dAlcEqu1CG6Z",
    "outputId": "754fff28-413e-4259-d1c4-591e23352fd0"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 400x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# instantiate test set according data loader\n",
    "test_set = DataSet(\"test\")\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=100)\n",
    "\n",
    "# collect feature magnitudes for\n",
    "known, known_unknown, unknown = [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "  # extract deep features magnitudes for validation set\n",
    "  for x,t in validation_loader:\n",
    "    # extract deep features (and logits)\n",
    "    logits, deep_feature = network(x.to(device))\n",
    "    # compute norms\n",
    "    nr = torch.norm(deep_feature, dim=1)\n",
    "    # split between known and unknown\n",
    "    k, kt, uk = split_known_unknown(nr, t)\n",
    "    # collect norms of known samples\n",
    "    known.extend(k.tolist())\n",
    "    # collect norms of known unknwown samples\n",
    "    known_unknown.extend(uk.tolist())\n",
    "\n",
    "  for x,t in test_loader:\n",
    "    # extract deep features (and logits)\n",
    "    _, f = network(x.to(device))\n",
    "    # compute norms\n",
    "    nr = torch.norm(f, dim=1)\n",
    "    # split between known and unknown\n",
    "    k, kt, uk = split_known_unknown(nr, t)\n",
    "    # collect norms of known samples\n",
    "    known.extend(k.tolist())\n",
    "    # collect norms of unknown unknown samples\n",
    "    unknown.extend(uk.tolist())\n",
    "\n",
    "# plot the norms as histograms\n",
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(4,2))\n",
    "\n",
    "# keep the same maximum magnitude\n",
    "max_mag = 20\n",
    "# plot the three histograms\n",
    "plt.hist(known, bins=100, range=(0,max_mag), density=True, color=\"g\", histtype=\"step\", label=\"Known\")\n",
    "plt.hist(known_unknown, bins=100, range=(0,max_mag), density=True, color=\"b\", histtype=\"step\", label=\"Known Unknown\")\n",
    "plt.hist(unknown, bins=100, range=(0,max_mag), density=True, color=\"r\", histtype=\"step\", label=\"Unknown Unknown\")\n",
    "\n",
    "# beautify plot\n",
    "plt.legend()\n",
    "plt.xlabel(\"Deep Feature Magnitude\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GUbbfV5WCG6a"
   },
   "source": [
    "## Classification Evaluation\n",
    "\n",
    "For a fixed threshold of $\\tau=0.98$, compute CCR and FPR for the test set.\n",
    "A well-trained network can achieve a CCR of > 90% for an FPR < 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fzUFBv0oCG6a",
    "outputId": "da688377-ea8d-460e-b935-ae4ae52b6328"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCR: 3519 of 3857 = 91.24%\n",
      "FPR: 173 of 2093 = 8.27%\n"
     ]
    }
   ],
   "source": [
    "from posixpath import split\n",
    "tau = 0.985\n",
    "\n",
    "# count the correctly classified and the total number of known samples\n",
    "correct = known = 0\n",
    "# count the incorrectly classified and the total number of unknown samples\n",
    "false = unknown = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "  for x,t in test_loader:\n",
    "    # extract logits (and deep features)\n",
    "    z, _ = network(x.to(device))\n",
    "    # compute softmax confidences\n",
    "    p = torch.nn.functional.softmax(z, dim=1).cpu()\n",
    "    # split between known and unknown\n",
    "    known_p, known_targets, unknown_p = split_known_unknown(p, t)\n",
    "\n",
    "    # compute number of correctly classified knowns above threshold\n",
    "    correct += sum(torch.logical_and(\n",
    "        torch.argmax(known_p, dim=1) == torch.argmax(known_targets, dim=1),\n",
    "        known_p[known_targets.bool()] > tau\n",
    "    ))\n",
    "    known += len(known_p)\n",
    "\n",
    "    # compute number of incorrectly accepted known samples\n",
    "    false += sum(torch.max(unknown_p, dim=1)[0] > tau)\n",
    "    unknown += len(t) - len(known_p)\n",
    "\n",
    "# print both rates\n",
    "print (f\"CCR: {correct} of {known} = {correct/known*100:2.2f}%\")\n",
    "print (f\"FPR: {false} of {unknown} = {false/unknown*100:2.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "interpreter": {
   "hash": "2dd53f8ad749bca69f7250ce75eb4f0def59db5cf79075a9716322ffc58e8a2e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
